=  Working with Virtual Machines

== Introduction

This section of our lab is dedicated to the Day-2 operations that many administrators would need to perform when working with virtual machines in their OpenShift Virtualization environment. Often this takes place by creating a load balancer object using a tool like *MetalLB*, and by exposing our application through *Services* and *Routes* to make it reachable from outside of the cluster. In this section we will cover the processes required to work with virtual machines once they are running within OpenShift virtualization 


// - Content from configure metalLB and assign IB to database node below

In this lab, you will review the MetalLB operator and expose virtual machine hosted applications outside of the cluster.

==  MetalLB concepts

Using MetalLB is valuable when you have a bare-metal cluster or a virtual infrastructure that is treated like bare-metal, and you want to ensure fault-tolerant access to an application through an external IP address.

For MetalLB to meet this need, you must configure your networking infrastructure to ensure that the network traffic for the external IP address is routed from clients to the host network for the cluster. 

It can operate in two modes:

* *MetalLB operating in layer2 mode* provides support for failover by utilizing a mechanism similar to IP failover. However, instead of relying on the virtual router redundancy protocol (VRRP) and keepalived, MetalLB leverages a gossip-based protocol to identify instances of node failure. When a failure is detected, another node assumes the role of the leader node, and a gratuitous ARP message is dispatched to broadcast this change.
* *MetalLB operating in layer3 or border gateway protocol (BGP) mode* delegates failure detection to the network. The _BGP_ router or routers that the *OpenShift Container Platform* nodes have established a connection with will identify any node failure and terminate the routes to that node.

Using MetalLB instead of IP failover is preferable for ensuring high availability of pods and services.

=== Layer2 mode

In layer 2 mode, the speaker pod on one node announces the external IP address for a service to the host network. From a network perspective, the node appears to have multiple IP addresses assigned to a network interface.

In layer 2 mode, all traffic for a service IP address is routed through one node. After traffic enters the node, the service proxy for the CNI network provider distributes the traffic to all the pods for the service.

When a node becomes unavailable, failover is automatic. The speaker pods on the other nodes detect that a node is unavailable, and a new speaker pod on a surviving node will take ownership of the service IP address from the failed node.

image::images/MetalLB/00_layer2.png[]

=== Layer 3 (BGP) mode

In BGP mode, by default, each speaker pod advertises the load balancer IP address for a service to each BGP peer. It is also possible to advertise the IPs coming from a given pool to a specific set of peers by adding an optional list of BGP peers. BGP peers are commonly network routers that are configured to use the BGP protocol. When a router receives traffic for the load balancer IP address, the router picks one of the nodes with a speaker pod that advertised the IP address. The router sends the traffic to that node. After traffic enters the node, the service proxy for the CNI network plugin distributes the traffic to all the pods for the service.

If a node becomes unavailable, the router then initiates a new connection with another node that has a speaker pod that is advertising the load balancer IP address.

image::images/MetalLB/00_bgp.png[]

////
== Review Operator

. Navigate to *Operators* -> *Installed Operators*. Select *All Projects* and select *MetalLB*
+
image::images/MetalLB/01_Operator_Installed.png[]

. Review the *Provided APIs* on the Details tab
+
image::images/MetalLB/02_Review_Operator.png[]

. Select the tab *MetalLB* to ensure the deployment is installed and configured correctly
+
image::images/MetalLB/03_Review_Operator_MetalLB.png[]
////

== Define IP AddressPool

For this lab, we will use the same network where the OpenShift Cluster nodes are located (`192.168.123.0/24`) and for this exercise we will reserve the IP range `192.168.123.200-192.168.123.250` to be used for load balanced services in the OpenShift cluster.

. In the left navigation menu, browse to *Operators -> Installed Operators*, switch to project `metallb-system`. Then choose the MetalLB Operator
+
image::images/MetalLB/40_navigation.png[]

. Switch the tab *IPAddressPool* (you may need to scroll the tabs to the right to see it) and press *Create IPAddressPool*
+
image::images/MetalLB/21_navigation.png[]

. Use the name `ip-addresspool-webapp` and under section _addresses_, remove any existing addresses and enter `192.168.123.200-192.168.123.250` as the address pool. When complete it should look similar to this image:
+
image::images/MetalLB/09_MetalLB_IPAddressPool_Defined.png[]

. Scroll down and press *Create*.

=== Configure Layer2 mode

For this lab we will use MetalLB in layer2 mode, so we need to create the configuration.

. Switch to the *L2Advertisement* tab (you may need to scroll the tab list to the right to see it) and press *Create L2Advertisement*.
+
image::images/MetalLB/22_navigation.png[]

. Indicate the name `l2-adv-webapp` and under section _ipaddressPools_ specify the value `ip-addresspool-webapp` as is shown:
+
image::images/MetalLB/10_MetalLB_L2Advertisement.png[]

. Press *Create*

== Expose the database node externally

If you completed the *Exposing apps using a Route* module, the VM is currently accessible from inside the cluster using the Service previously created. In this task, we will expose port 3306 outside of the cluster, making the database available to other virtual machines and consumers not hosted in OpenShift.

. Navigate to *Networking* -> *Services* and select the project `vmexamples`
+
image::images/MetalLB/11_Services.png[]
+
[IMPORTANT]
====
If you did not complete the module *Migrating Virtual Machines* you can use pre-existing virtual machines in the `vmimported` project. 

If you are using the pre-imported virtual machines, please replace all instances of `vmexamples` namespace with `vmimported`.
====

. Press *Create Service* and fill the form with the following code snippet:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: database-metallb
  namespace: vmexamples
spec:
  type: LoadBalancer
  selector:
    vm.kubevirt.io/name: database
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
----
+
[NOTE]
Notice the `type` indicated is `LoadBalancer`. Since this cluster has MetalLB installed, it will result in the specified port(s) exposed using that. There are other load balancer options available from partners such as F5, Nginx, and more.

. Press *Create* and review the *Service* created. Notice the IP address assigned to the load balancer is from the range specified earlier in the lab.
+
image::images/MetalLB/12_Service_created.png[]

. To verify connectivity to the database service via the external IP, open the web terminal by clicking the following icon in the right-top part.
+
image::images/OCP_Terminal_Icon.png[]

. A console in the bottom part of the screen appears
+
image::images/OCP_Terminal.png[]

. Using the right console, try to access the IP assigned and the port 3306
+
[%nowrap]
----
[~] $ curl -s 192.168.123.202:3306 | cut -c1-16       
----
+
.Sample Output
+
[%nowrap]
----
5.5.68-MariaDB
----

== Summary 

MetalLB is a straightforward and simple solution for bare-metal, on-premises deployments to expose applications outside of the cluster, without the need to configure physical networks with NMstate or multus.



















// - Content from Service/route Module Below

== Exposing an Application with a Service/Route

By default, virtual machines are connected to the SDN, which is a convenient and easy way to give them access to the rest of the network, but can be challenging for the virtual machines, and other Pods in the OpenShift cluster, to find and connect to the virtualized applications. To solve this, we will use a `Service` to balance connections across the two Windows-based web servers, and create a DNS entry for each service discovery, then create a `Route` to allow external clients to access the application hosted within the virtual machines.


[IMPORTANT]
====
If you have not completed the module *Migrating Virtual Machines*, it is recommended that you do that module first. However, you can use pre-existing virtual machines that have been imported automatically in the `vmimported` project. 

If you are using the pre-imported virtual machines, please replace all instances of `vmexamples` namespace with `vmimported`.
====

== Using a Service and Route to expose an application
////
=== Create the Service

The `Service` identifies the source/target for traffic, and directs clients to, the endpoints based on labels. Currently, the VMs do not have a label assigned yet.

In order to successfully associate the  VMs with the Service, we need to do the following:

* Add a label to the VMs. We will use the same label for both Windows IIS servers because they are both behind the same load balancer.
* Create the service to make the two Windows IIS servers available for other workloads on the cluster. OpenShift will automatically make the load balancer internally accessible using the name of the Service as the DNS name.
* Make the service available outside of OpenShift by creating a *Route*.

To begin, we'll add labels to the virtual machines by modifying their definition in the OpenShift Virtualization GUI.

==== Label the virtual machines

. From the OpenShift console, navigate to *Virtualization* -> *VirtualMachines* and ensure the migrated VMs successfully imported and are running.
+
image::images/MTV/60_VMWARE_VMs_List.png[]
+
[NOTE]
====
Ensure you select the correct project, `vmexamples` if you completed the *Migrating Virtual Machines* module or `vmimported` if you did not.
====

. Select to the `winweb01` VM and navigate to the *YAML* tab.
+
image::images/MTV/204_label_navigation.png[]

. Find the `spec:` section and under the `template.metadata` add the following lines to label the VM resources:
+
[%nowrap]
----
      labels:
        env: webapp
----
+
[IMPORTANT]
====
Make sure to get the indentation exactly right - just like in the screenshot below.
====
+
image::images/MTV/61_VMWARE_VMs_YAML.png[]

. *Repeat* the process for the VM `winweb02`.

. Start, or restart if already running, the _Virtual Machines_ `database`, `winweb01` and `winweb02`
.. Ensure the VMs are properly working by accessing to the console tab of each VM.

==== Create the Service

. Navigate to *Networking* -> *Services* and press *Create Service*. 
+
image::images/MTV/200_navigate_service.png[]
+
Remember the label that you added to your VMs (`env=webapp`)? The Service will use that label in its selector to pick which VMs to route traffic to.
. Replace the YAML with the following definition
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: webapp
  namespace: vmexamples
spec:
  selector:
    env: webapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
----
+
[IMPORTANT]
====
Ensure the namespace with your virtual machines, `vmexamples` or `vmimported`, is the one used in the Service YAML.
====
+
image::images/MTV/201_service_yaml.png[]

. Press *Create*.

. From the details page for the newly created `webapp` Service, locate *Pod selector* link and click it.
+
image::images/MTV/62_00_VMWARE_VMs_podSelector.png[]

. Verify the two Windows VMs are properly identified and targeted by the Service.
+
image::images/MTV/62_01_VMWARE_VMs_podSelector_verification.png[]
+
[NOTE]
====
What if the VMs are not in this list? There are a few things to verify and double check. At any time, summon a proctor for help if you prefer.

. Ensure that the label applied to the VMs and the selector used by the Service match.
. If the virtual machines were already running, ensure they were restarted after updating the `VirtualMachine` YAML with the label.
. Verify that the label was applied to the correct YAML section in the `VirtualMachine` definition. It should be under `spec.template.metadata`.
. In the left navigation menu, browse to *Workloads -> Pods*, select `virt-launcher` Pod with the virtual machine's name in it. On the ensuing details page, verify the `env=webapp` label is present in the list.
+
image::images/MTV/202_label_troubleshooting_1.png[]
====

=== Create the Route

Now the Windows IIS servers are accessible from within the OpenShift cluster. Other virtual machines are able to access them using the DNS name `webapp.vmexamples`, which is determined by the name of the Service + the namespace. However, since these web servers are the front end to an application we want to be externally accessible, we will expose it using a *Route*.

. Navigate to *Networking* -> *Routes* in the left navigation menu, verify that you're using the correct project name. Press *Create Route*.
+
image::images/MTV/205_route_navigation.png[]

. Fill the form using the information below, press *Create* when done.
+
.. *Name*: `route-webapp`
.. *Service*: `webapp`
.. *Target port*: `80 -> 80 (TCP)`
+
[NOTE]
====
OpenShift can automatically (re)encrypt traffic entering the cluster via a Route, however, we don't need to use TLS for this application. The *Secure Route* option should not be checked.
====
+
image::images/MTV/63_VMWARE_VMs_Create_Route.png[]
////
. Navigate to the address shown in *Location* field
+
image::images/MTV/203_route_access.png[]

. When the page loads, you will see an error. This is because the Windows web servers running on VMware are not able to currently connect to the database VM after it's migration.
+
image::images/MTV/64_VMWARE_VMs_URL.png[]
+
To fix the connectivity issue, we need to create a Service for the database VM and then create a Route so that it is reachable from outside of the OpenShift cluster.

. Navigate to *Networking* -> *Services* and press *Create service*. Replace the YAML with the following definition:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: vmexamples
spec:
  selector:
    vm.kubevirt.io/name: database
  ports:
    - protocol: TCP
      port: 3306
      targetPort: 3306
----
+
[IMPORTANT]
====
Ensure the namespace with your virtual machines, `vmexamples` or `vmimported` is the one used in the Service YAML.
====
+
////
[NOTE]
====
In this example the service is simply using a selector of the VM's name. This is a default label that is automatically added to all VMs. Since there is only one VM that matches the selector, the service will not load balance to the database, instead we're using the Service for discovery via the internal DNS name.
====
////
. We now need to create a route from which the database can be accessed outside of the cluster.
//ADD STEPS TO CREATE ROUTE AND DISCOVER FROM WINDOWS VMS
. Reload the webapp URL and expect to get the proper result
+
image::images/MTV/65_VMWARE_VMs_URL.png[]
